{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yIKQ-LR5nBk"
      },
      "source": [
        "## CS431 Data Intensive Distributed Analytics\n",
        "### Fall 2024 - Assignment 3\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCp5Oj1e5nBn"
      },
      "source": [
        "**Please edit this (text) cell to provide your name and UW student ID number!**\n",
        "* **Name:** Ananya Trivedi\n",
        "* **ID:** 20874394"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdL9NkD55nBo"
      },
      "source": [
        "---\n",
        "#### Overview\n",
        "For this assignment, you will be using Python and Spark to perform some graph analysis, using a graph of the Gnutella server network.   In this graph, each node represents a server, and each (directed) edge represents a connection between servers in Gnutella's peer-to-peer network.  The input file for this assignment, `p2p-Gnutella08-adj.txt`, represents the graph as an adjacency list.   Each server (node) is identified by a unique number, and each line in the file gives the adjacency list for a single server.\n",
        "For example, this line:\n",
        "> 91\t243\t1923\t2194\n",
        "\n",
        "gives the adjacency list for server `91`.   It indicates that there are edges from server `91` to servers `243`, `1923`, and `2194`.    According to the Stanford Network Analysis Project, which collected these data, [the graph includes 6301 servers and 20777 edges](http://snap.stanford.edu/data/p2p-Gnutella08.html).\n",
        "\n",
        "Run the following block to install Spark and download the input file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3PNw3CWW5nBp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20990f3e-fadb-4fbf-fbe4-f9b28596648a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ],
      "source": [
        "!apt-get update -qq > /dev/null\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!curl -Os https://student.cs.uwaterloo.ca/~cs451/spark/spark-3.4.3-bin-hadoop3.tgz\n",
        "!tar xzf spark-3.4.3-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "!curl -Os https://student.cs.uwaterloo.ca/~cs451/content/cs431/p2p-Gnutella08-adj.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O396xBwYgu5C"
      },
      "source": [
        "### Or\n",
        "Run the following block to download just the input file (for those running on their own machine)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kn5akOogu5D"
      },
      "outputs": [],
      "source": [
        "!curl -Os https://student.cs.uwaterloo.ca/~cs451/content/cs431/p2p-Gnutella08-adj.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5juGVRT5gu5D"
      },
      "source": [
        "set the environment variables (needed on Colab, shouldn't be needed on your own machine - and if it is, these values are likely not correct!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "h6dNbvPEgu5D"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.3-bin-hadoop3\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rnFWG115nBq"
      },
      "source": [
        "and then create a `SparkContext`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0kx1nY435nBr"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "spark_conf = SparkConf()\\\n",
        "  .setAppName(\"YourTest\")\\\n",
        "  .setMaster(\"local[*]\")\n",
        "\n",
        "sc = SparkContext.getOrCreate(spark_conf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgYYXO41gu5D"
      },
      "source": [
        "#### Colab code to view the Spark UI\n",
        "This box will definitely not work on your own machine, which is fine, on your own machine the box above will have given you a local link - ctrl+click should open it in most editor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "sE0rMCPNgu5D",
        "outputId": "6ad97893-57ca-4ceb-a920-b982fe603b76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mWarning: This function may stop working due to changes in browser security.\n",
            "Try `serve_kernel_port_as_iframe` instead. \u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, text, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port);\n",
              "    const anchor = document.createElement('a');\n",
              "    anchor.href = new URL(path, url).toString();\n",
              "    anchor.target = '_blank';\n",
              "    anchor.setAttribute('data-href', url + path);\n",
              "    anchor.textContent = text;\n",
              "    element.appendChild(anchor);\n",
              "  })(4040, \"/jobs/index.html\", \"https://localhost:4040/jobs/index.html\", window.element)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import output\n",
        "output.serve_kernel_port_as_window(4040, path='/jobs/index.html')\n",
        "# This will create a link below. You must click the link, do not copy & paste the URL as that's the \"local\" URL and won't work on your machine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsQFCPO85nBs"
      },
      "source": [
        "---\n",
        "## Important\n",
        "### Watch Your Types\n",
        "###### The questions that follow ask you to implement functions whose prototypes are given to you. Do **NOT** change the prototypes of the functions.\n",
        "###### This means: Do not change the type of value returned, do not add or remove parameters.\n",
        "### No Top-Level Expressions\n",
        "###### Do **NOT** write code outside of functions. All necessary code should be included in the function body (except for import statements). You may declare functions inside of the function body. When marking, we will execute your code by calling the functions from an external program, which is why your code cannot rely on statements running outside functions. Please remove any call to the functions that you may have introduced for test purposes before submitting your notebook.  Better still, create new cells for your tests, then you do not need to remember to delete anything\n",
        "### Dummy Values\n",
        "###### The function prototypes all return a default value, labelled as a dummy.  This is so that the cell is runnable even if you do not do a question.  You should replace this value with the correct value.\n",
        "### New Cells\n",
        "###### The best way to test your code is to create a NEW code cell from which to call your functions.  We will **ONLY** run the code cells that **WE** provided.  So do not put any helper functions in these cells, unless the helper is **ONLY** for use with your tests.  Any helpers needed by your solutions themselves must go in the original code cell.  (All cells will be put into a single .py file, so you do not need to copy and paste a helper from one question to the next)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2xfj_3e5nBt"
      },
      "source": [
        "---\n",
        "#### Question 1  (6/24 marks):\n",
        "\n",
        "To get warmed up, you should first write Spark code to confirm or determine some basic properties of the Gnutella graph.  Write code in the provided functions that will return answers to the following questions, as specified in each function's documentation:\n",
        "- How many nodes and edges are there in the graph?  (This should confirm the numbers given above.)\n",
        "- How many nodes of each outdegree are there? That is, how many nodes have no outgoing edges, how many have one outgoing edge, how many have two outgoing edges, and so on?\n",
        "- How many nodes of each indegree are there?\n",
        "\n",
        "You should use Spark to answer these questions.   Do *not* load the entire graph into your Python driver program."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "TnoJMCQY5nBu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2845f1fe-f691-4502-e967-f5304041024b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{74: 1,\n",
              " 70: 3,\n",
              " 2: 1287,\n",
              " 4: 559,\n",
              " 6: 227,\n",
              " 54: 1,\n",
              " 50: 1,\n",
              " 82: 1,\n",
              " 20: 4,\n",
              " 30: 1,\n",
              " 10: 37,\n",
              " 32: 2,\n",
              " 62: 2,\n",
              " 8: 76,\n",
              " 14: 13,\n",
              " 12: 23,\n",
              " 18: 2,\n",
              " 72: 2,\n",
              " 60: 3,\n",
              " 56: 2,\n",
              " 16: 1,\n",
              " 66: 2,\n",
              " 52: 1,\n",
              " 38: 1,\n",
              " 86: 1,\n",
              " 44: 1,\n",
              " 22: 1,\n",
              " 78: 1,\n",
              " 1: 2452,\n",
              " 59: 1,\n",
              " 49: 1,\n",
              " 7: 144,\n",
              " 5: 333,\n",
              " 11: 29,\n",
              " 71: 3,\n",
              " 81: 4,\n",
              " 19: 2,\n",
              " 13: 19,\n",
              " 67: 3,\n",
              " 21: 2,\n",
              " 57: 1,\n",
              " 47: 2,\n",
              " 3: 868,\n",
              " 9: 70,\n",
              " 15: 8,\n",
              " 69: 2,\n",
              " 25: 1,\n",
              " 33: 1,\n",
              " 31: 2,\n",
              " 77: 2,\n",
              " 73: 2,\n",
              " 51: 1,\n",
              " 85: 1,\n",
              " 83: 1,\n",
              " 87: 1,\n",
              " 61: 1,\n",
              " 27: 1,\n",
              " 35: 1,\n",
              " 63: 1,\n",
              " 79: 1,\n",
              " 55: 1,\n",
              " 41: 1,\n",
              " 91: 1,\n",
              " 23: 1}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# Q1\n",
        "\n",
        "def num_nodes_edges():\n",
        "    \"\"\"Returns a tuple (num_nodes, num_edges)\"\"\"\n",
        "    # load data into an RDD\n",
        "    rdd = sc.textFile(\"p2p-Gnutella08-adj.txt\")\n",
        "\n",
        "    nodes_edges = rdd.map(lambda line: line.strip().split())\n",
        "    nodes = nodes_edges.map(lambda x: x[0]).distinct().count()\n",
        "    edges = nodes_edges.map(lambda x: len(x) - 1).sum()\n",
        "\n",
        "    return (nodes, edges)\n",
        "\n",
        "def out_counts():\n",
        "    \"\"\"Returns a dictionary where the keys are the outdegrees, and the\n",
        "    values are the number of nodes of the corresponding outdegree \"\"\"\n",
        "        # load data into an RDD\n",
        "    rdd = sc.textFile(\"p2p-Gnutella08-adj.txt\")\n",
        "    outdegree_counts = rdd.map(lambda line: line.strip().split()) \\\n",
        "                           .map(lambda x: (x[0], len(x) - 1)) \\\n",
        "                           .map(lambda x: (x[1], 1)) \\\n",
        "                           .reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "    # convert to dictionary\n",
        "    outdegree_dict = dict(outdegree_counts.collect())\n",
        "    return outdegree_dict\n",
        "\n",
        "\n",
        "def in_counts():\n",
        "    \"\"\"Returns a dictionary where the keys are the indegrees, and the\n",
        "    values are the number of nodes of the corresponding indegree \"\"\"\n",
        "    # flatten out each edge as (neighbor, 1)\n",
        "    rdd = sc.textFile(\"p2p-Gnutella08-adj.txt\")\n",
        "    indegree_counts = rdd.flatMap(lambda line: [(neighbor, 1) for neighbor in line.strip().split()[1:]]) \\\n",
        "                          .reduceByKey(lambda a, b: a + b) \\\n",
        "                          .map(lambda x: (x[1], 1)) \\\n",
        "                          .reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "    # convert to dictionary\n",
        "    indegree_dict = dict(indegree_counts.collect())\n",
        "    return indegree_dict\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNp6bYDY5nBv"
      },
      "source": [
        "---\n",
        "Your main objective for this assignment is to perform *single source personalized page rank* over the Gnutella graph.  To get started, you should make sure that you have a clear understanding of ordinary (i.e., non-personalized) page rank.  Read the description of page rank in Section 5.3 of [the course textbook](https://lintool.github.io/MapReduceAlgorithms/index.html).   Personalized page rank is like ordinary page rank except:\n",
        "- One node in the graph is designated as the *source* node. Personalized page rank is performed with respect to that source node.\n",
        "- Personalized page rank is initialized by assigning all probability mass to the source node, and none to the other nodes. In contrast, ordinary page rank is initialized by giving all nodes the same probability mass.\n",
        "- Whenever personalized page rank makes a random jump, it jumps back to the source node. In contrast, ordinary page rank may jump to any node.\n",
        "- In personalized page rank, all probability mass lost dangling nodes is put back into the source nodes.  In ordinary page rank, lost mass is distributed evenly over all nodes.\n",
        "\n",
        "#### Testing Tips\n",
        "\n",
        "Look at the first few lines of the input file. Try using node 0 as the source node and run it for 1 iteration. The source node should have `jump_factor` for its rank, and its out neighbours should all have the same value - `(1 - jump_factor) / degree(node 0)`\n",
        "\n",
        "Once that works, try doing the math for 2 iterations and then confirm that works, too. Make sure it's handling all cases correctly.\n",
        "\n",
        "You might also consider making a smaller input file to test on so the math is easier to do.\n",
        "\n",
        "#### Question 2  (10/24 marks):\n",
        "\n",
        "Your task is to write a Spark program to perform personalized page rank over the Gnutella graph for a specified number of iterations, and of course a specific node. The function you will implement takes three input values:\n",
        "- source node id (a non-negative integer)\n",
        "- iteration count (a positive integer)\n",
        "- random jump factor value (a float between 0 and 1) - This is 1-B as introduced in the lecture.\n",
        "\n",
        "The function should perform personalized page rank, with respect to the specified source node, over the Gnutella graph, for the specified number of iterations, using Spark.\n",
        "The output of your function should be a list of the 10 nodes with the highest personalized page rank with respect to the given source. For each of the 10 nodes, return the node's id and page rank value as a tuple. The list returned by the function should therefore look something like this: `[(node_id_1, highest_pagerank_value), ..., (node_id_10, 10th_highest_pagerank_value)]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "7oo6ny035nBv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2564a7cc-4e60-4960-eb96-1a7d852c4322"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('0', 1.0),\n",
              " ('1', 0.0),\n",
              " ('2', 0.0),\n",
              " ('3', 0.0),\n",
              " ('4', 0.0),\n",
              " ('5', 0.0),\n",
              " ('6', 0.0),\n",
              " ('7', 0.0),\n",
              " ('8', 0.0),\n",
              " ('9', 0.0)]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# Q2\n",
        "\n",
        "def personalized_page_rank(source_node_id, num_iterations, jump_factor):\n",
        "    \"\"\"Returns a list of the 10 nodes with the highest page rank value along with their value, as tuples\n",
        "    [(node_id_1, highest_pagerank_value), ..., (node_id_10, 10th_highest_pagerank_value)]\"\"\"\n",
        "\n",
        "    # load data into an RDD\n",
        "    rdd = sc.textFile(\"p2p-Gnutella08-adj.txt\")\n",
        "    # parse the data into (node, list of neighbors) pairs\n",
        "    neighbors_list = rdd.map(lambda line: line.strip().split()) \\\n",
        "                .map(lambda nodes: (nodes[0], nodes[1:])) \\\n",
        "                .cache()\n",
        "    # source code rank is 1, all others are 0\n",
        "    ranks = neighbors_list.mapValues(lambda _: 0).map(lambda x: (x[0], 1.0 if x[0] == str(source_node_id) else 0.0))\n",
        "\n",
        "    # PageRank algorithm\n",
        "    for _ in range(num_iterations):\n",
        "        # contributions to neighbors\n",
        "        contributions = neighbors_list.join(ranks) \\\n",
        "                             .flatMap(lambda x: [(neighbor, x[1][1] / len(x[1][0])) for neighbor in x[1][0]])\n",
        "        # calculate new ranks, (with jump factor dangling nodes)\n",
        "        ranks = contributions.reduceByKey(lambda a, b: a + b) \\\n",
        "                             .mapValues(lambda rank: jump_factor if rank == 0 else rank * (1 - jump_factor) + jump_factor)\n",
        "        # lost mass\n",
        "        lost_mass = 1.0 - ranks.values().sum()\n",
        "        ranks = ranks.mapValues(lambda rank: rank + lost_mass if rank == source_node_id else rank)\n",
        "\n",
        "    # top 10\n",
        "    top_10 = ranks.takeOrdered(10, key=lambda x: -x[1])\n",
        "    return top_10\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmZAajwV5nBw"
      },
      "source": [
        "---\n",
        "#### Question 3  (4/24 marks):\n",
        "\n",
        "For the previous question, you implemented personalized page rank that ran for a specified number of iterations.  However, it is also common to write iterative algorithms that run until some specified termination condition is reached.\n",
        "For example, for page rank, suppose the $p_i(x)$ represents the probability mass assigned to node $x$ after the $i$th iteration of the algorithm.  ($p_0(x)$ is the initial probability mass of node $x$.)   We define the change of $x$'s probability mass on the $i$th iteration as $\\lvert p_i(x)-p_{i-1}(x) \\rvert$.   Then, we can iterate personalized page rank until the maximum (over all nodes) change is less than a specified threshold, i.e, until all nodes' page ranks have converged.\n",
        "\n",
        "For this question, modify your personalized page rank implementation from Question 2 so that it iterates until the\n",
        "maximum node change is less than $\\frac{0.5}{N}$, where $N$ represents the number of nodes in the graph.\n",
        "This version of the function should take only two inputs: the source node id and the random jump factor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "whRf8BnZ5nBx"
      },
      "outputs": [],
      "source": [
        "# Q3\n",
        "\n",
        "def personalized_page_rank_stopping_criterion(source_node_id, jump_factor):\n",
        "    \"\"\"Returns a list of the 10 nodes with the highest page rank value along with their value, as tuples\n",
        "    [(node_id_1, highest_pagerank_value), ..., (node_id_10, 10th_highest_pagerank_value)]\"\"\"\n",
        "\n",
        "    # load data into an RDD\n",
        "    rdd = sc.textFile(\"/path/to/p2p-Gnutella08-adj.txt\")\n",
        "    change_limit = float(\"inf\")\n",
        "\n",
        "    # parse the data into (node, list of neighbors) pairs\n",
        "    neighbors_list = rdd.map(lambda line: line.strip().split()) \\\n",
        "                .map(lambda nodes: (nodes[0], nodes[1:])) \\\n",
        "                .cache() # CACHE\n",
        "    ranks = neighbors_list.mapValues(lambda _: 0).map(lambda x: (x[0], 1.0 if x[0] == str(source_node_id) else 0.0)).cache()\n",
        "    num_nodes = neighbors_list.count()\n",
        "    threshold = 0.5 / num_nodes\n",
        "\n",
        "  # calculate page rannk until convergence\n",
        "    while change_limit > threshold:\n",
        "        # join links with ranks to distribute ranks to neighbors\n",
        "        contributions = neighbors_list.join(ranks) \\\n",
        "                             .flatMap(lambda x: [(neighbor, x[1][1] / len(x[1][0])) for neighbor in x[1][0]])\n",
        "        # calculate new ranks\n",
        "        new_ranks = contributions.reduceByKey(lambda a, b: a + b) \\\n",
        "                                 .mapValues(lambda rank: rank * (1 - jump_factor) + jump_factor)\\\n",
        "                                 .cache() # CACHE\n",
        "        # lost mass\n",
        "        lost_mass = 1.0 - new_ranks.values().sum()\n",
        "        new_ranks = new_ranks.mapValues(lambda rank: rank + lost_mass if rank == source_node_id else rank)\n",
        "        # calculate the maximum change in ranks for convergence check\n",
        "        rank_changes = ranks.join(new_ranks) \\\n",
        "                            .mapValues(lambda x: abs(x[1] - x[0]))\\\n",
        "                            .cache() # CACHE\n",
        "        max_change = rank_changes.values().max()\n",
        "        # update ranks for iterations\n",
        "        ranks = new_ranks\n",
        "\n",
        "    # top 10\n",
        "    top_10 = ranks.takeOrdered(10, key=lambda x: -x[1])\n",
        "\n",
        "    return top_10\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suYKj6_S5nBy"
      },
      "source": [
        "---\n",
        "#### Question 4  (4/24 marks):\n",
        "\n",
        "Spark provides the ability to *cache* RDDs.   This is useful when an RDD will be used more than once.   Instead of computing the same RDD multiple times, it can be computed once, cached, and then re-used from the cache.   Read about caching in the Spark textbook, or in the [persistence section of the Spark RDD programming guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence).   Caching can be particularly effective for iterative Spark applications, like those you are writing for this assignment.\n",
        "\n",
        "For this question, go back to the code that you wrote to answer Question 3, and add caching.   (Caching will not affect the functionality of your code, i.e., what it computes.   It should only affect performance.)   Don't worry about different persistence levels.   Just use `cache()`, which caches RDDs at the default persistence level.\n",
        "\n",
        "In addition to adding `cache()` calls in your code, use the text cell below to briefly explain how you decided which RDDs to cache.\n",
        "\n",
        "If you were not able to finish Question 3, add caching annotations to your solution for Question 2 instead.\n",
        "\n",
        "IMPORTANT: Because the files are small, the OS will retain the intermediate shuffle files so you won't actually see any improvement in you running times! (This means you have to think about what to cache rather than trying various places to trial and error it)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_fRhytv5nBy"
      },
      "source": [
        "---\n",
        "#### Your answer to Question 4:\n",
        "\n",
        "We cached neighbors_list as it is used in every iteration to join with the ranks rdd. We do not want to recompute neighbors_list every single iteration. Similarly, we also cached the ranks rdd as it is repeatedly used in contributions and rank_chnages. We also cached contributions as it is used multiple times in each iteration to form new_ranks. Finally we cached rank_changes as it is used to compute change_limit in eat iteration (which determines when the loop terminates)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AAWthLp5nBz"
      },
      "source": [
        "---\n",
        "Don't forget to save your workbook!   When you are finished and you are ready to submit your assignment, download your notebook file (.ipynb) from the hub to your machine, and then follow the submission instructions in the assignment."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}